{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain requires python version ≥ 3.8.1. i installed 3.8.16.\n",
    "#!pip3 -q install langchain\n",
    "#!pip3 -q install gitpython\n",
    "#!pip3 -q install deeplake\n",
    "#!pip3 -q install tiktoken\n",
    "#!pip3 -q install llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "70b6e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, tokenize, re, os, requests, fnmatch, argparse, base64\n",
    "\n",
    "# documentation: https://api.python.langchain.com/en/latest/\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.docstore.document import Document\n",
    "# from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import GitLoader, TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from git import Repo\n",
    "\n",
    "# llama_index and llama_hub both libraries are full of errors.\n",
    "# # documentation: https://gpt-index.readthedocs.io/en/latest/\n",
    "# from llama_index import download_loader\n",
    "# from llama_hub.github_repo import GithubRepositoryReader, GithubClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ee105",
   "metadata": {},
   "source": [
    "### Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519a0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/1769332/script-to-remove-python-comments-docstrings\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    # source => # import os\\n# import sys\\n# sys.path.insert(0, os.path.abspath(\\'.\\'))\\n\\nimport toml\\n\\nwith open ...\n",
    "    io_obj = io.StringIO(source)\n",
    "    out = \"\"\n",
    "    prev_toktype = tokenize.INDENT\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    for tok in tokenize.generate_tokens(io_obj.readline):\n",
    "        token_type = tok[0]\n",
    "        token_string = tok[1]\n",
    "        start_line, start_col = tok[2]\n",
    "        end_line, end_col = tok[3]\n",
    "        ltext = tok[4]\n",
    "        if start_line > last_lineno:\n",
    "            last_col = 0\n",
    "        if start_col > last_col:\n",
    "            out += (\" \" * (start_col - last_col))\n",
    "        if token_type == tokenize.COMMENT:\n",
    "            pass\n",
    "        elif token_type == tokenize.STRING:\n",
    "            if prev_toktype != tokenize.INDENT:\n",
    "                if prev_toktype != tokenize.NEWLINE:\n",
    "                    if start_col > 0:\n",
    "                        out += token_string\n",
    "        else:\n",
    "            out += token_string\n",
    "        prev_toktype = token_type\n",
    "        last_col = end_col\n",
    "        last_lineno = end_line\n",
    "    out = '\\n'.join(l for l in out.splitlines() if l.strip())\n",
    "    # out => 'import toml\\nwith open(\"../../pyproject.toml\") as f:\\n    data = toml.load(f)\\nproject = \"ð\\x9f¦\\x9cð\\x9f\\x94\\x97 LangCha ...\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c919fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bfdf64",
   "metadata": {},
   "source": [
    "### Main Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GITHUB_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bd98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48399498/git-executable-not-found-in-python\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"#export GIT_PYTHON_REFRESH=quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9cdcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repo.config_writer().set_value(\"user\", \"name\", \"myusername\").release()\n",
    "#repo.config_writer().set_value(\"pack\", \"window\", 0).release()\n",
    "\n",
    "# os.system(\"git config --global user.name \\\"firstname lastname\\\"\")\n",
    "os.system(\"git config --global pack.window 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2e3d7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "190435a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1156"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = Repo.clone_from(\n",
    "    \"https://github.com/hwchase17/langchain\", to_path=\"./example_data/test_repo1\"\n",
    ")\n",
    "\n",
    "# branch = repo.head.reference\n",
    "# loader = GitLoader(repo_path=\"./example_data/test_repo1\", branch=branch, file_filter=lambda file_path: file_path.endswith(\".py\"),)\n",
    "# data = loader.load()\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "393235b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.Document'>\n",
      "1157\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./example_data/test_repo1\"\n",
    "\n",
    "docs = []\n",
    "i=0\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):    \n",
    "    for file in filenames:       \n",
    "        # type(file) => <class 'str'>\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:            \n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")# encoding=\"utf-8\"  # https://docs.python.org/3/library/codecs.html#standard-encodings              \n",
    "                # docs.extend(loader.load_and_split())\n",
    "                docs.extend(loader.load())\n",
    "            except:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"unicode_escape\")# encoding=\"utf-8\"                \n",
    "                # docs.extend(loader.load_and_split())\n",
    "                docs.extend(loader.load()) \n",
    "#     i+=1\n",
    "#     if i==3: break\n",
    "\n",
    "str_doc = ''\n",
    "for doc in docs:\n",
    "    str_doc = str_doc + ' ' + remove_comments_and_docstrings(doc.page_content)\n",
    "docs = docs[0:1]    \n",
    "docs[0].page_content = str_doc\n",
    "docs[0].metadata['source'] = ''\n",
    "\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a58e8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0].page_content[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ef857764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "#text_splitter = CharacterTextSplitter(separator = \"\\n\\n\", chunk_size=1000, chunk_overlap=0, length_function = len)#chunk_size=1000, chunk_overlap=0\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = [\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=12000, chunk_overlap=0, length_function = len)# [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "23665bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937bea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTY4NzA2MjIwMiwiZXhwIjoxNjkwNzc3MzgwfQ.eyJpZCI6Imt1bXJuYXJlbmRlciJ9._RRtUmMv5CbE89y5jFUKt86JFxYHUEJ3tdimouUmQbt2Xpj1NuBIoRsRLfMKzP4QtSu162icg4-QVDY3qQhxGA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed258e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base='', openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4338f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./deeplake_data/test_repo1 loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake_data/test_repo1', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (259, 1536)  float32   None   \n",
      "    ids      text     (259, 1)      str     None   \n",
      " metadata    json     (259, 1)      str     None   \n",
      "   text      text     (259, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.deeplake.DeepLake at 0x2366eed5820>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.activeloop.ai/storage-and-credentials/storage-options\n",
    "DEEPLAKE_ACCOUNT_NAME = \"kumrnarender\"\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path='./deeplake_data/test_repo1',#dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/test_repo1\"\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4b7124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./deeplake_data/test_repo1 loaded successfully.\n",
      "\n",
      "Deep Lake Dataset in ./deeplake_data/test_repo1 already exists, loading from the storage\n",
      "Dataset(path='./deeplake_data/test_repo1', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape      dtype  compression\n",
      "  -------   -------    -------    -------  ------- \n",
      " embedding  generic  (259, 1536)  float32   None   \n",
      "    ids      text     (259, 1)      str     None   \n",
      " metadata    json     (259, 1)      str     None   \n",
      "   text      text     (259, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "\r"
     ]
    }
   ],
   "source": [
    "db = DeepLake(dataset_path='./deeplake_data/test_repo1', read_only=True, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ea44fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(db) => <class 'langchain.vectorstores.deeplake.DeepLake'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "049fa52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example for semantic search query in the vector database.\n",
    "# query = \"count the approximate number of executable code lines or operations?\"\n",
    "# zzz = db.similarity_search(query)\n",
    "# print(zzz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cef0dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "\n",
    "# fetch_k => specify how many documents to fetch before filtering.\n",
    "# fetch_k be set to a value greater than k.\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "# k => the number of documents that will be returned after filtering.\n",
    "retriever.search_kwargs['k'] = 10\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "# type(retriever) => <class 'langchain.vectorstores.base.VectorStoreRetriever'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d480a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOpenAI(model='gpt-4') # 'gpt-3.5-turbo',\n",
    "# qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# max_tokens => tokens in the the completion.\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0, max_tokens=1000)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "# max_tokens_limit => tokens in the messages.\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever, max_tokens_limit=12000)\n",
    "# print(type(qa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49808679",
   "metadata": {},
   "source": [
    "InvalidRequestError: This model's maximum context length is 16385 tokens. However, you requested 31308 tokens (14924 in the messages, 16384 in the completion). Please reduce the length of the messages or completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "800db161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question**: From class hierarchy, which one is most complex?\n",
      "**Answer**: The most complex class in the hierarchy is the `ConversationSummaryBufferMemory` class, which is a subclass of both `BaseChatMemory` and `SummarizerMixin`. It inherits functionality from both parent classes and combines them to provide a memory that can store chat messages and generate summaries of the conversation. \n",
      "\n",
      "\n",
      "**Question**: Determine the cyclomatic complexity of the class \n",
      "**Answer**: The cyclomatic complexity of the class `ConversationSummaryBufferMemory` is 7. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "#     \"fetch the code.\",\n",
    "#     \"Get all the classes which are derived from the base class?\",\n",
    "    \"From class hierarchy, which one is most complex?\",\n",
    "    \"Determine the cyclomatic complexity of the class \",\n",
    "]\n",
    "chat_history = []\n",
    "j=0\n",
    "for question in questions:\n",
    "    if j==0: result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    else: result = qa({\"question\": question+re.search('`([^`]*)`', result[\"answer\"]).group(0), \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"**Question**: {question}\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\\n\")\n",
    "    j+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5e14e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_history[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0540ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
